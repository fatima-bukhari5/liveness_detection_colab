{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatima-bukhari5/liveness_detection_colab/blob/main/Liveness_VGG16_Copy1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27af8f73",
      "metadata": {
        "id": "27af8f73"
      },
      "source": [
        "## Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2e3311ab",
      "metadata": {
        "id": "2e3311ab"
      },
      "outputs": [],
      "source": [
        "# Reference section to call important lib.\n",
        "\n",
        "# mathemathical & data manupulation\n",
        "import numpy as np          # numpy for mathemathical operations\n",
        "import pandas as pd         # to manage data in two dimension structures\n",
        "\n",
        "# O/S, command line, serialization, deserialization, iterations\n",
        "import argparse             # command line arguments management.\n",
        "import pickle               # pickle for object serialization/de-serialization\n",
        "import os                   # to interact with operating system\n",
        "import itertools            # to enhance iteration over object, i.e. for loop\n",
        "\n",
        "# image processing\n",
        "import cv2                  # opencv for image pre-processing\n",
        "from imutils import paths   # another convienance for image processing like opencv\n",
        "\n",
        "# For Data visualization & charts\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Reference to Scikit Learn for data pre-processing & post training\n",
        "# performance evaluation. \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Reference to keras with tensorflow engine for model training.\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Dense, Flatten\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.layers.convolutional import *\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73100f8d",
      "metadata": {
        "id": "73100f8d"
      },
      "source": [
        "### Importing Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "31d70324",
      "metadata": {
        "id": "31d70324"
      },
      "outputs": [],
      "source": [
        "INIT_LR = 0.0001  # a variable for learning rate parameter while training\n",
        "BS = 10           # ??????\n",
        "EPOCHS = 16        # a variable for number of epochs parameter while training model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_-2X_GJJ3lh",
        "outputId": "1bc91fae-1a74-43f5-87b9-b10a22caa394"
      },
      "id": "U_-2X_GJJ3lh",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "bf7a92e2",
      "metadata": {
        "id": "bf7a92e2"
      },
      "outputs": [],
      "source": [
        "# location for training, validation & testing dataset \n",
        "train_path = '/content/drive/MyDrive/real_and_fake/train'\n",
        "valid_path = '/content/drive/MyDrive/real_and_fake/valid'\n",
        "test_path = '/content/drive/MyDrive/real_and_fake/test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "87e481aa",
      "metadata": {
        "id": "87e481aa"
      },
      "outputs": [],
      "source": [
        "def read_image_data(X,Y):\n",
        "    '''\n",
        "    The following function is to convert the data into numpy array through\n",
        "    `data_processing` function and resize it according vgg16 input size \n",
        "    of 224 by 224.\n",
        "    \n",
        "    Requirement:  Convert input images to 224x224 for VGG16\n",
        "    Usage:        Pre-Processing.\n",
        "    Functionality:\n",
        "                  1. load images one by one from folder, resize & attach lable.\n",
        "                  2. Convert image to numpy array & return array with lable. \n",
        "    Parameters:\n",
        "                  X => df['Image paths'] (dataframe column of image paths)\n",
        "                  Y => df['Labels'] (dataframe column of image labels)\n",
        "\n",
        "    Returns:      `image` in the form of numpy array & labels as encoded.\n",
        "    '''\n",
        "    # to do: variables & parmeters naming convention.\n",
        "    # declaration Section\n",
        "    data =[]                                      # An array to hold images.\n",
        "    labels = Y                                    # A variable to hold label from parameter.\n",
        "    # execution section    \n",
        "    # to do: exception handling\n",
        "    for path in X:                                # for each image in a given folder.\n",
        "        image = cv2.imread(path)                  # reading image from given path.\n",
        "        image = cv2.resize(image,(224,224))       # resizing image as of input requirement.\n",
        "        data.append(image)                        # add image to declared array structure.\n",
        "    data, labels = data_processing(data, labels)  # Calling & passing parameters to data_processing function.\n",
        "    # to do: use python log api for console message, if any. print is childish.\n",
        "    print('[INFO] Converting Data into Image form and encoding labels')\n",
        "    # return section\n",
        "    return data, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f27e579d",
      "metadata": {
        "id": "f27e579d"
      },
      "outputs": [],
      "source": [
        "def data_organization(img_path):\n",
        "    '''\n",
        "    This function is to organize the data in the form of data frames to have clear understanding \n",
        "    and readability of data w.r.t to its labels\n",
        "\n",
        "    Parameters: \n",
        "    img_path = folder path of images base folder as string\n",
        "\n",
        "    Returns: dataframe with organized image paths and their labels\n",
        "    '''\n",
        "    print(\"Getting labels for images...\")\n",
        "    data = []\n",
        "    labels = []\n",
        "    list_image_path = []\n",
        "    folders_list = os.listdir(img_path)\n",
        "    \n",
        "    # loop over all folders at a time. i.e real and fake\n",
        "    for folder in folders_list:\n",
        "        \n",
        "        # extract the class label from the filename to add into the df\n",
        "    \n",
        "        folder_path = os.path.join(img_path,folder)\n",
        "        images_list = os.listdir(folder_path)\n",
        "        for image_name in images_list:\n",
        "            label = folder\n",
        "            imagePath = os.path.join(folder_path,image_name)\n",
        "            labels.append(label)\n",
        "            list_image_path.append(imagePath)\n",
        "            \n",
        "    image_path_label = {'image_path': list_image_path,'labels':labels}\n",
        "    df_image_path_label = pd.DataFrame(image_path_label)\n",
        "    \n",
        "    print(\"DataFrame Complete....\")\n",
        "    return df_image_path_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2ee6bfb9",
      "metadata": {
        "id": "2ee6bfb9"
      },
      "outputs": [],
      "source": [
        "def data_processing(data,labels):\n",
        "    '''\n",
        "    Data processing to convert images data to numpy along with label encoding\n",
        "    \n",
        "    Parameters:\n",
        "    data -> list of images data\n",
        "    labels -> list of labels\n",
        "    \n",
        "    returns:\n",
        "    data, labels as numpy array of image data and encoded labels\n",
        "    '''\n",
        "    data = np.array(data, dtype=\"float\")\n",
        "    le = LabelEncoder()\n",
        "    labels = le.fit_transform(labels)\n",
        "    labels = to_categorical(labels, 2)\n",
        "    \n",
        "    return data, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a85f96f8",
      "metadata": {
        "id": "a85f96f8"
      },
      "source": [
        "## Build Fine-tuned VGG16 model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f93d681e",
      "metadata": {
        "id": "f93d681e"
      },
      "source": [
        "VGG16 Model was used in an ImageNet competition where thousands of images were classified into different categories. We'll be using this model as a baseline and fine tune it as to our specific requirements. \n",
        "VGG16 in itself is a model that classifies images into a number of classes that are not limited to just two. In our case the output layer requires just two nodes representing two classes as `real` and `fake`. So, we will be tuning the model accordingly. \n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Importing the model will take time as its a file of size almost 17 MB "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "1c3ab0c0",
      "metadata": {
        "id": "1c3ab0c0"
      },
      "outputs": [],
      "source": [
        "def model_training(train_data, train_labels, val_data, val_labels):\n",
        "    '''\n",
        "    The function is involved with \n",
        "        - data augmentation \n",
        "        - sequential model initialization\n",
        "        - using vgg16 model layers as base layers for the new model.\n",
        "        - compiling the model\n",
        "        - training the model\n",
        "    \n",
        "    returns a trained model file\n",
        "    '''\n",
        "    # construct the training image generator for data augmentation\n",
        "    aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "        width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
        "        horizontal_flip=True, fill_mode=\"nearest\")\n",
        "   \n",
        "    print(\"[INFO] importing VGG16 \")\n",
        "    vgg16_model = keras.applications.vgg16.VGG16(include_top=True)\n",
        "    \n",
        "    # getting a sequential model and freezing the previous\n",
        "    model = Sequential()\n",
        "    for layer in vgg16_model.layers[:-1]:\n",
        "        model.add(layer)\n",
        "    \n",
        "    # After poping the last layer, freeze the previous layers and adding a new dense output layer\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = False\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    \n",
        "    # initialize the optimizer and model\n",
        "    print(\"[INFO] compiling model...\")\n",
        "    opt = Adam(lr=INIT_LR)\n",
        "    \n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "    \n",
        "    # train the network\n",
        "    print(\"[INFO] training network for {} epochs...\".format(EPOCHS))\n",
        "    \n",
        "    model.fit(x=aug.flow(train_data, train_labels, batch_size=BS),\n",
        "        validation_data=(val_data, val_labels), validation_steps=5, steps_per_epoch=len(train_data) / BS,\n",
        "        epochs=EPOCHS)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4040f21a",
      "metadata": {
        "id": "4040f21a"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, df_testing):\n",
        "    '''\n",
        "    Getting predictions w.r.t to the trained model as passed and the training data\n",
        "    \n",
        "    Parameters:\n",
        "    model: trained model\n",
        "    df_testing: data for testing in the form of dataframe\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    Predictions as 0 or 1 and true_labels after data processing\n",
        "    '''\n",
        "    \n",
        "    # getting test data in numpy \n",
        "    test_data, true_labels = read_image_data(df_testing['image_path'], df_testing['labels']) \n",
        "    \n",
        "    # gettin predictions against the model\n",
        "    print('[INFO] Getting Predictions')\n",
        "    predictions = model.predict(test_data, steps=1, verbose=0)\n",
        "    predictions_rounded = np.round(predictions)\n",
        "    \n",
        "    return predictions_rounded, true_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "79d86f72",
      "metadata": {
        "id": "79d86f72"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "dd3f2ab5",
      "metadata": {
        "id": "dd3f2ab5"
      },
      "outputs": [],
      "source": [
        "def model_performance(predictions, true_labels):\n",
        "    '''\n",
        "    The model displays a confusion matrix using a previously built confusion matrix function taken from sklearn.\n",
        "    \n",
        "    Parameters:\n",
        "    predictions: previous predictions\n",
        "    true_labels: array of actual labels to be compared\n",
        "    \n",
        "    Result: dislay confusion matrix\n",
        "    '''\n",
        "    print('[INFO] Getting Confusion Matrix')\n",
        "    cm = confusion_matrix(true_labels[:,0], predictions[:,0])\n",
        "    \n",
        "\n",
        "    cm_plot_labels = ['real','fake']\n",
        "    plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "1184c7ab",
      "metadata": {
        "id": "1184c7ab"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    '''\n",
        "    calling all functions\n",
        "    '''\n",
        "    \n",
        "    # getting labels to each data set\n",
        "    df_training = data_organization(train_path)\n",
        "    df_validation = data_organization(valid_path)\n",
        "    df_testing = data_organization(test_path)\n",
        "    \n",
        "    # getting train and validation data in numpy \n",
        "    train_data, train_labels = read_image_data(df_training['image_path'], df_training['labels'])\n",
        "    val_data, val_labels = read_image_data(df_validation['image_path'], df_validation['labels'])\n",
        "    \n",
        "    # get the trained model\n",
        "    model = model_training(train_data=train_data, train_labels=train_labels, \n",
        "                           val_data=val_data, val_labels=val_labels)\n",
        "    \n",
        "    # get the results\n",
        "    predictions, true_labels = get_predictions(model = model, df_testing=df_testing)\n",
        "    \n",
        "    #\n",
        "    model_performance(predictions=predictions, true_labels=true_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "653c17be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "653c17be",
        "outputId": "b21d048c-c7f0-45fa-d24e-d882226c3d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting labels for images...\n",
            "DataFrame Complete....\n",
            "Getting labels for images...\n",
            "DataFrame Complete....\n",
            "Getting labels for images...\n",
            "DataFrame Complete....\n",
            "[INFO] Converting Data into Image form and encoding labels\n",
            "[INFO] Converting Data into Image form and encoding labels\n",
            "[INFO] importing VGG16 \n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 3s 0us/step\n",
            "553476096/553467096 [==============================] - 3s 0us/step\n",
            "[INFO] compiling model...\n",
            "[INFO] training network for 16 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/16\n",
            "30/30 [==============================] - 18s 221ms/step - loss: 0.9415 - accuracy: 0.4633 - val_loss: 0.6749 - val_accuracy: 0.7100\n",
            "Epoch 2/16\n",
            "30/30 [==============================] - 4s 144ms/step - loss: 0.7793 - accuracy: 0.5900 - val_loss: 0.4354 - val_accuracy: 0.8800\n",
            "Epoch 3/16\n",
            "30/30 [==============================] - 4s 146ms/step - loss: 0.6380 - accuracy: 0.6933 - val_loss: 0.3283 - val_accuracy: 0.9500\n",
            "Epoch 4/16\n",
            "30/30 [==============================] - 4s 147ms/step - loss: 0.5746 - accuracy: 0.7333 - val_loss: 0.2675 - val_accuracy: 0.9500\n",
            "Epoch 5/16\n",
            "30/30 [==============================] - 4s 142ms/step - loss: 0.5234 - accuracy: 0.7733 - val_loss: 0.2200 - val_accuracy: 0.9600\n",
            "Epoch 6/16\n",
            "30/30 [==============================] - 4s 143ms/step - loss: 0.4663 - accuracy: 0.8100 - val_loss: 0.2127 - val_accuracy: 0.9400\n",
            "Epoch 7/16\n",
            "30/30 [==============================] - 4s 146ms/step - loss: 0.4597 - accuracy: 0.8333 - val_loss: 0.1954 - val_accuracy: 0.9400\n",
            "Epoch 8/16\n",
            "30/30 [==============================] - 4s 142ms/step - loss: 0.4553 - accuracy: 0.8033 - val_loss: 0.2147 - val_accuracy: 0.9300\n",
            "Epoch 9/16\n",
            "30/30 [==============================] - 4s 143ms/step - loss: 0.4552 - accuracy: 0.8100 - val_loss: 0.2027 - val_accuracy: 0.9300\n",
            "Epoch 10/16\n",
            "30/30 [==============================] - 4s 146ms/step - loss: 0.4232 - accuracy: 0.8267 - val_loss: 0.1871 - val_accuracy: 0.9500\n",
            "Epoch 11/16\n",
            "30/30 [==============================] - 4s 143ms/step - loss: 0.3853 - accuracy: 0.8333 - val_loss: 0.1924 - val_accuracy: 0.9500\n",
            "Epoch 12/16\n",
            "30/30 [==============================] - 4s 145ms/step - loss: 0.3915 - accuracy: 0.8333 - val_loss: 0.2013 - val_accuracy: 0.9400\n",
            "Epoch 13/16\n",
            "30/30 [==============================] - 4s 145ms/step - loss: 0.3595 - accuracy: 0.8533 - val_loss: 0.1842 - val_accuracy: 0.9500\n",
            "Epoch 14/16\n",
            "30/30 [==============================] - 4s 146ms/step - loss: 0.3670 - accuracy: 0.8433 - val_loss: 0.1787 - val_accuracy: 0.9500\n",
            "Epoch 15/16\n",
            "30/30 [==============================] - 4s 145ms/step - loss: 0.3529 - accuracy: 0.8533 - val_loss: 0.1879 - val_accuracy: 0.9500\n",
            "Epoch 16/16\n",
            "30/30 [==============================] - 4s 150ms/step - loss: 0.3296 - accuracy: 0.8800 - val_loss: 0.1491 - val_accuracy: 0.9700\n",
            "[INFO] Converting Data into Image form and encoding labels\n",
            "[INFO] Getting Predictions\n",
            "[INFO] Getting Confusion Matrix\n",
            "Confusion matrix, without normalization\n",
            "[[94  6]\n",
            " [ 1 28]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mobile Net"
      ],
      "metadata": {
        "id": "56mbBEMbeZMQ"
      },
      "id": "56mbBEMbeZMQ"
    },
    {
      "cell_type": "code",
      "source": [
        "mobile = keras.applications.mobilenet.MobileNet()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfj4pPcFedmP",
        "outputId": "bad6939f-d800-491a-dffe-fd3574a7357f"
      },
      "id": "jfj4pPcFedmP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf.h5\n",
            "17227776/17225924 [==============================] - 0s 0us/step\n",
            "17235968/17225924 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mobile.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJsdM9dQepFx",
        "outputId": "2bc0262d-d1d4-44fd-ea27-dd656f7a9c9e"
      },
      "id": "kJsdM9dQepFx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"mobilenet_1.00_224\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv1 (Conv2D)              (None, 112, 112, 32)      864       \n",
            "                                                                 \n",
            " conv1_bn (BatchNormalizatio  (None, 112, 112, 32)     128       \n",
            " n)                                                              \n",
            "                                                                 \n",
            " conv1_relu (ReLU)           (None, 112, 112, 32)      0         \n",
            "                                                                 \n",
            " conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)     288       \n",
            "                                                                 \n",
            " conv_dw_1_bn (BatchNormaliz  (None, 112, 112, 32)     128       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_dw_1_relu (ReLU)       (None, 112, 112, 32)      0         \n",
            "                                                                 \n",
            " conv_pw_1 (Conv2D)          (None, 112, 112, 64)      2048      \n",
            "                                                                 \n",
            " conv_pw_1_bn (BatchNormaliz  (None, 112, 112, 64)     256       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_pw_1_relu (ReLU)       (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " conv_pad_2 (ZeroPadding2D)  (None, 113, 113, 64)      0         \n",
            "                                                                 \n",
            " conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)       576       \n",
            "                                                                 \n",
            " conv_dw_2_bn (BatchNormaliz  (None, 56, 56, 64)       256       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_dw_2_relu (ReLU)       (None, 56, 56, 64)        0         \n",
            "                                                                 \n",
            " conv_pw_2 (Conv2D)          (None, 56, 56, 128)       8192      \n",
            "                                                                 \n",
            " conv_pw_2_bn (BatchNormaliz  (None, 56, 56, 128)      512       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_pw_2_relu (ReLU)       (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)      1152      \n",
            "                                                                 \n",
            " conv_dw_3_bn (BatchNormaliz  (None, 56, 56, 128)      512       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_dw_3_relu (ReLU)       (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv_pw_3 (Conv2D)          (None, 56, 56, 128)       16384     \n",
            "                                                                 \n",
            " conv_pw_3_bn (BatchNormaliz  (None, 56, 56, 128)      512       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_pw_3_relu (ReLU)       (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv_pad_4 (ZeroPadding2D)  (None, 57, 57, 128)       0         \n",
            "                                                                 \n",
            " conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)      1152      \n",
            "                                                                 \n",
            " conv_dw_4_bn (BatchNormaliz  (None, 28, 28, 128)      512       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_dw_4_relu (ReLU)       (None, 28, 28, 128)       0         \n",
            "                                                                 \n",
            " conv_pw_4 (Conv2D)          (None, 28, 28, 256)       32768     \n",
            "                                                                 \n",
            " conv_pw_4_bn (BatchNormaliz  (None, 28, 28, 256)      1024      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_pw_4_relu (ReLU)       (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)      2304      \n",
            "                                                                 \n",
            " conv_dw_5_bn (BatchNormaliz  (None, 28, 28, 256)      1024      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_dw_5_relu (ReLU)       (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv_pw_5 (Conv2D)          (None, 28, 28, 256)       65536     \n",
            "                                                                 \n",
            " conv_pw_5_bn (BatchNormaliz  (None, 28, 28, 256)      1024      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_pw_5_relu (ReLU)       (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv_pad_6 (ZeroPadding2D)  (None, 29, 29, 256)       0         \n",
            "                                                                 \n",
            " conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)      2304      \n",
            "                                                                 \n",
            " conv_dw_6_bn (BatchNormaliz  (None, 14, 14, 256)      1024      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_dw_6_relu (ReLU)       (None, 14, 14, 256)       0         \n",
            "                                                                 \n",
            " conv_pw_6 (Conv2D)          (None, 14, 14, 512)       131072    \n",
            "                                                                 \n",
            " conv_pw_6_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_pw_6_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)      4608      \n",
            "                                                                 \n",
            " conv_dw_7_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_dw_7_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_pw_7 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "                                                                 \n",
            " conv_pw_7_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_pw_7_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)      4608      \n",
            "                                                                 \n",
            " conv_dw_8_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_dw_8_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_pw_8 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "                                                                 \n",
            " conv_pw_8_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_pw_8_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)      4608      \n",
            "                                                                 \n",
            " conv_dw_9_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_dw_9_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_pw_9 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "                                                                 \n",
            " conv_pw_9_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " conv_pw_9_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_dw_10 (DepthwiseConv2D  (None, 14, 14, 512)      4608      \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv_dw_10_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " conv_dw_10_relu (ReLU)      (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_pw_10 (Conv2D)         (None, 14, 14, 512)       262144    \n",
            "                                                                 \n",
            " conv_pw_10_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " conv_pw_10_relu (ReLU)      (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_dw_11 (DepthwiseConv2D  (None, 14, 14, 512)      4608      \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv_dw_11_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " conv_dw_11_relu (ReLU)      (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_pw_11 (Conv2D)         (None, 14, 14, 512)       262144    \n",
            "                                                                 \n",
            " conv_pw_11_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " conv_pw_11_relu (ReLU)      (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)      0         \n",
            "                                                                 \n",
            " conv_dw_12 (DepthwiseConv2D  (None, 7, 7, 512)        4608      \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv_dw_12_bn (BatchNormali  (None, 7, 7, 512)        2048      \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " conv_dw_12_relu (ReLU)      (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv_pw_12 (Conv2D)         (None, 7, 7, 1024)        524288    \n",
            "                                                                 \n",
            " conv_pw_12_bn (BatchNormali  (None, 7, 7, 1024)       4096      \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " conv_pw_12_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
            "                                                                 \n",
            " conv_dw_13 (DepthwiseConv2D  (None, 7, 7, 1024)       9216      \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv_dw_13_bn (BatchNormali  (None, 7, 7, 1024)       4096      \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " conv_dw_13_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
            "                                                                 \n",
            " conv_pw_13 (Conv2D)         (None, 7, 7, 1024)        1048576   \n",
            "                                                                 \n",
            " conv_pw_13_bn (BatchNormali  (None, 7, 7, 1024)       4096      \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " conv_pw_13_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 1, 1, 1024)       0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1, 1, 1024)        0         \n",
            "                                                                 \n",
            " conv_preds (Conv2D)         (None, 1, 1, 1000)        1025000   \n",
            "                                                                 \n",
            " reshape_2 (Reshape)         (None, 1000)              0         \n",
            "                                                                 \n",
            " predictions (Activation)    (None, 1000)              0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,253,864\n",
            "Trainable params: 4,231,976\n",
            "Non-trainable params: 21,888\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TINkijlaey8q"
      },
      "id": "TINkijlaey8q",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Liveness VGG16-Copy1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}