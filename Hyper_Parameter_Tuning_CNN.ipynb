{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyper_Parameter_Tuning_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhsF79Bz5UDJzfcErBAPHz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatima-bukhari5/liveness_detection_colab/blob/main/Hyper_Parameter_Tuning_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Library Imports\n"
      ],
      "metadata": {
        "id": "0Mabfiiw1gvu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RCTFDWyhSUxO"
      },
      "outputs": [],
      "source": [
        "# Reference section to call important lib.\n",
        "\n",
        "# mathemathical & data manupulation\n",
        "import numpy as np          # numpy for mathemathical operations\n",
        "import pandas as pd         # to manage data in two dimension structures\n",
        "\n",
        "# O/S, command line, serialization, deserialization, iterations\n",
        "import argparse             # command line arguments management.\n",
        "import pickle               # pickle for object serialization/de-serialization\n",
        "import os                   # to interact with operating system\n",
        "import itertools            # to enhance iteration over object, i.e. for loop\n",
        "\n",
        "# image processing\n",
        "import cv2                  # opencv for image pre-processing\n",
        "from imutils import paths   # another convienance for image processing like opencv\n",
        "\n",
        "# For Data visualization & charts\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Reference to Scikit Learn for data pre-processing & post training\n",
        "# performance evaluation. \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Reference to keras with tensorflow engine for model training.\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Dense, Flatten\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.layers.convolutional import *\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF1S4zw2-zIq",
        "outputId": "b462e064-84ff-436f-ed3b-be7cd933b998"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INIT_LR = 0.0001  # a variable for learning rate parameter while training\n",
        "BS = 10           # batch size\n",
        "EPOCHS = 16       # a variable for number of epochs parameter while training model"
      ],
      "metadata": {
        "id": "wQu0HBXlDBvM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# location for training, validation & testing dataset \n",
        "train_path = '/content/drive/MyDrive/real_and_fake/train'\n",
        "valid_path = '/content/drive/MyDrive/real_and_fake/valid'\n",
        "test_path = '/content/drive/MyDrive/real_and_fake/test'"
      ],
      "metadata": {
        "id": "E-vd6r7DDR_N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_image_data(X,Y, height, width):\n",
        "    '''\n",
        "    The following function is to convert the data into numpy array through\n",
        "    `data_processing` function and resize it according vgg16 input size \n",
        "    of 224 by 224.\n",
        "    \n",
        "    Requirement:  Convert input images to heightxwidth for VGG16\n",
        "    Usage:        Pre-Processing.\n",
        "    Functionality:\n",
        "                  1. load images one by one from folder, resize & attach lable.\n",
        "                  2. Convert image to numpy array & return array with lable. \n",
        "    Parameters:\n",
        "                  X => df['Image paths'] (dataframe column of image paths)\n",
        "                  Y => df['Labels'] (dataframe column of image labels)\n",
        "\n",
        "    Returns:      `image` in the form of numpy array & labels as encoded.\n",
        "    '''\n",
        "    # to do: variables & parmeters naming convention.\n",
        "    # declaration Section\n",
        "    data =[]                                      # An array to hold images.\n",
        "    labels = Y                                    # A variable to hold label from parameter.\n",
        "    # execution section    \n",
        "    # to do: exception handling\n",
        "    for path in X:                                # for each image in a given folder.\n",
        "        image = cv2.imread(path)                  # reading image from given path.\n",
        "        image = cv2.resize(image,(height,width))       # resizing image as of input requirement.\n",
        "        data.append(image)                        # add image to declared array structure.\n",
        "    data, labels = data_processing(data, labels)  # Calling & passing parameters to data_processing function.\n",
        "    # to do: use python log api for console message, if any. print is childish.\n",
        "    print('[INFO] Converting Data into Image form and encoding labels')\n",
        "    # return section\n",
        "    return data, labels"
      ],
      "metadata": {
        "id": "VvZVMIhpDWv2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_organization(img_path):\n",
        "    '''\n",
        "    This function is to organize the data in the form of data frames to have clear understanding \n",
        "    and readability of data w.r.t to its labels\n",
        "\n",
        "    Parameters: \n",
        "    img_path = folder path of images base folder as string\n",
        "\n",
        "    Returns: dataframe with organized image paths and their labels\n",
        "    '''\n",
        "    print(\"Getting labels for images...\")\n",
        "    data = []\n",
        "    labels = []\n",
        "    list_image_path = []\n",
        "    folders_list = os.listdir(img_path)\n",
        "    \n",
        "    # loop over all folders at a time. i.e real and fake\n",
        "    for folder in folders_list:\n",
        "        \n",
        "        # extract the class label from the filename to add into the df\n",
        "    \n",
        "        folder_path = os.path.join(img_path,folder)\n",
        "        images_list = os.listdir(folder_path)\n",
        "        for image_name in images_list:\n",
        "            label = folder\n",
        "            imagePath = os.path.join(folder_path,image_name)\n",
        "            labels.append(label)\n",
        "            list_image_path.append(imagePath)\n",
        "            \n",
        "    image_path_label = {'image_path': list_image_path,'labels':labels}\n",
        "    df_image_path_label = pd.DataFrame(image_path_label)\n",
        "    \n",
        "    print(\"DataFrame Complete....\")\n",
        "    return df_image_path_label"
      ],
      "metadata": {
        "id": "i5ie2-TCDr2Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_processing(data,labels):\n",
        "    '''\n",
        "    Data processing to convert images data to numpy along with label encoding\n",
        "    \n",
        "    Parameters:\n",
        "    data -> list of images data\n",
        "    labels -> list of labels\n",
        "    \n",
        "    returns:\n",
        "    data, labels as numpy array of image data and encoded labels\n",
        "    '''\n",
        "    data = np.array(data, dtype=\"float\")\n",
        "    le = LabelEncoder()\n",
        "    labels = le.fit_transform(labels)\n",
        "    labels = to_categorical(labels, 2)\n",
        "    \n",
        "    return data, labels"
      ],
      "metadata": {
        "id": "9BheSHRhDwUz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(model, df_testing, height, width):\n",
        "    '''\n",
        "    Getting predictions w.r.t to the trained model as passed and the training data\n",
        "    \n",
        "    Parameters:\n",
        "    model: trained model\n",
        "    df_testing: data for testing in the form of dataframe\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    Predictions as 0 or 1 and true_labels after data processing\n",
        "    '''\n",
        "    \n",
        "    # getting test data in numpy \n",
        "    test_data, true_labels = read_image_data(df_testing['image_path'], df_testing['labels'], height, width) \n",
        "    \n",
        "    # gettin predictions against the model\n",
        "    print('[INFO] Getting Predictions')\n",
        "    predictions = model.predict(test_data, steps=1, verbose=0)\n",
        "    predictions_rounded = np.round(predictions)\n",
        "    \n",
        "    return predictions_rounded, true_labels"
      ],
      "metadata": {
        "id": "AnvVWRjgD2FU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "sdbYHSZBEHwC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_performance(predictions, true_labels):\n",
        "    '''\n",
        "    The model displays a confusion matrix using a previously built confusion matrix function taken from sklearn.\n",
        "    \n",
        "    Parameters:\n",
        "    predictions: previous predictions\n",
        "    true_labels: array of actual labels to be compared\n",
        "    \n",
        "    Result: dislay confusion matrix\n",
        "    '''\n",
        "    print('[INFO] Getting Confusion Matrix')\n",
        "    cm = confusion_matrix(true_labels[:,0], predictions[:,0])\n",
        "    \n",
        "\n",
        "    cm_plot_labels = ['real','fake']\n",
        "    plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')\n",
        "    "
      ],
      "metadata": {
        "id": "-UOhsvrBEVZB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(num_classes, height, width, model_name):\n",
        "    img_augmentation = Sequential(\n",
        "    [\n",
        "        layers.RandomRotation(factor=0.15),\n",
        "        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
        "        layers.RandomFlip(),\n",
        "        layers.RandomContrast(factor=0.1),\n",
        "    ],\n",
        "    name=\"img_augmentation\",)\n",
        "    initializer = tf.keras.initializers.GlorotNormal(seed = None)\n",
        "    inputs = layers.Input(shape=(height, width, 3,))\n",
        "    x = img_augmentation(inputs)\n",
        "\n",
        "    # importing the requires model\n",
        "    if model_name == \"effnet_v2l\":\n",
        "      model = keras.applications.efficientnet_v2.EfficientNetV2L(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
        "    elif model_name == \"effnet_b2\":\n",
        "      model = keras.applications.efficientnet.EfficientNetB7(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
        "    elif model_name == \"nas_net\":\n",
        "      model = keras.applications.nasnet.NASNetLarge(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
        "    elif model_name == \"inc_resnet_v2\":\n",
        "      model = keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
        "\n",
        "    # Freeze the pretrained weights\n",
        "    model.trainable = False\n",
        "\n",
        "    # Rebuild top\n",
        "    x = layers.Dense(3, kernel_initializer=initializer, activation='relu')\n",
        "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    top_dropout_rate = 0.2\n",
        "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
        "    outputs = layers.Dense(2, activation=\"softmax\", name=\"pred\")(x)\n",
        "\n",
        "    # Compile\n",
        "    model = tf.keras.Model(inputs, outputs, name=model_name)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
        "    model.compile(\n",
        "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "p22_JyM_Ej2G"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_get_predictions():\n",
        "  df_training = data_organization(train_path)\n",
        "  df_validation = data_organization(valid_path)\n",
        "  df_testing = data_organization(test_path)\n",
        "\n",
        "  all_models = ['effnet_v2l', 'effnet_b2', 'nas_net', 'inc_resnet_v2']\n",
        "\n",
        "  train_data, train_labels = read_image_data(df_training['image_path'], df_training['labels'],224, 224)\n",
        "  val_data, val_labels = read_image_data(df_validation['image_path'], df_validation['labels'], 224, 224)\n",
        "\n",
        "  all_predictions = {}\n",
        "  for model_name in all_models:\n",
        "    model = build_model(num_classes = 2, height=224, width=224, model_name=model_name)\n",
        "    print(\"[INFO] Training model \", model_name)\n",
        "    hist = model.fit(x=train_data, y=train_labels, validation_data=(val_data, val_labels), verbose=2, batch_size = BS,  \n",
        "          validation_steps=5, steps_per_epoch=len(train_data) / BS, epochs=20)\n",
        "\n",
        "    # get predictions\n",
        "    predictions, true_labels = get_predictions(model = model, df_testing=df_testing, height = 224, width = 224)\n",
        "    all_predictions[model_name] = {\"predictions\": predictions,\n",
        "                                   \"true_labels\": true_labels, \n",
        "                                   \"hist\": hist}\n",
        "  return all_predictions\n"
      ],
      "metadata": {
        "id": "s-kJjTPaFPqZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_hist(hist):\n",
        "    plt.plot(hist.history[\"accuracy\"])\n",
        "    plt.plot(hist.history[\"val_accuracy\"])\n",
        "    plt.title(\"model accuracy\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BucZxBC-GDCY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = train_model_get_predictions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEZvHuAcFTlr",
        "outputId": "d2ae1955-3cec-47c5-b329-27031dce0544"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting labels for images...\n",
            "DataFrame Complete....\n",
            "Getting labels for images...\n",
            "DataFrame Complete....\n",
            "Getting labels for images...\n",
            "DataFrame Complete....\n",
            "[INFO] Converting Data into Image form and encoding labels\n",
            "[INFO] Converting Data into Image form and encoding labels\n",
            "[INFO] Training model  effnet_v2l\n",
            "Epoch 1/20\n",
            "30/30 - 221s - loss: 1.2264 - accuracy: 0.7100 - val_loss: 0.0365 - val_accuracy: 1.0000 - 221s/epoch - 7s/step\n",
            "Epoch 2/20\n",
            "30/30 - 190s - loss: 0.9493 - accuracy: 0.8133 - val_loss: 1.1211 - val_accuracy: 0.5200 - 190s/epoch - 6s/step\n",
            "Epoch 3/20\n",
            "30/30 - 189s - loss: 1.0957 - accuracy: 0.8167 - val_loss: 0.1440 - val_accuracy: 0.9200 - 189s/epoch - 6s/step\n",
            "Epoch 4/20\n",
            "30/30 - 193s - loss: 0.9914 - accuracy: 0.8133 - val_loss: 0.0345 - val_accuracy: 0.9800 - 193s/epoch - 6s/step\n",
            "Epoch 5/20\n",
            "30/30 - 193s - loss: 1.1856 - accuracy: 0.7967 - val_loss: 0.2697 - val_accuracy: 0.8800 - 193s/epoch - 6s/step\n",
            "Epoch 6/20\n",
            "30/30 - 196s - loss: 0.8237 - accuracy: 0.8267 - val_loss: 0.2434 - val_accuracy: 0.9200 - 196s/epoch - 7s/step\n",
            "Epoch 7/20\n",
            "30/30 - 192s - loss: 1.0203 - accuracy: 0.8233 - val_loss: 0.1282 - val_accuracy: 0.9600 - 192s/epoch - 6s/step\n",
            "Epoch 8/20\n",
            "30/30 - 193s - loss: 1.1346 - accuracy: 0.8200 - val_loss: 0.7819 - val_accuracy: 0.8200 - 193s/epoch - 6s/step\n",
            "Epoch 9/20\n",
            "30/30 - 193s - loss: 0.9808 - accuracy: 0.8100 - val_loss: 0.6991 - val_accuracy: 0.8800 - 193s/epoch - 6s/step\n",
            "Epoch 10/20\n",
            "30/30 - 193s - loss: 1.0693 - accuracy: 0.8100 - val_loss: 0.0471 - val_accuracy: 0.9800 - 193s/epoch - 6s/step\n",
            "Epoch 11/20\n",
            "30/30 - 190s - loss: 1.1601 - accuracy: 0.8033 - val_loss: 0.7425 - val_accuracy: 0.8000 - 190s/epoch - 6s/step\n",
            "Epoch 12/20\n",
            "30/30 - 193s - loss: 0.8464 - accuracy: 0.8367 - val_loss: 0.5880 - val_accuracy: 0.8800 - 193s/epoch - 6s/step\n",
            "Epoch 13/20\n",
            "30/30 - 192s - loss: 0.9036 - accuracy: 0.8633 - val_loss: 0.5977 - val_accuracy: 0.8600 - 192s/epoch - 6s/step\n",
            "Epoch 14/20\n",
            "30/30 - 193s - loss: 0.9952 - accuracy: 0.8133 - val_loss: 0.1542 - val_accuracy: 0.9600 - 193s/epoch - 6s/step\n",
            "Epoch 15/20\n",
            "30/30 - 192s - loss: 1.1926 - accuracy: 0.8400 - val_loss: 0.5023 - val_accuracy: 0.9000 - 192s/epoch - 6s/step\n",
            "Epoch 16/20\n",
            "30/30 - 190s - loss: 0.9533 - accuracy: 0.8500 - val_loss: 0.5687 - val_accuracy: 0.9200 - 190s/epoch - 6s/step\n",
            "Epoch 17/20\n",
            "30/30 - 191s - loss: 0.9906 - accuracy: 0.8633 - val_loss: 0.8405 - val_accuracy: 0.8800 - 191s/epoch - 6s/step\n",
            "Epoch 18/20\n",
            "30/30 - 190s - loss: 1.0858 - accuracy: 0.8200 - val_loss: 0.8179 - val_accuracy: 0.9000 - 190s/epoch - 6s/step\n",
            "Epoch 19/20\n",
            "30/30 - 191s - loss: 0.9757 - accuracy: 0.8400 - val_loss: 0.4367 - val_accuracy: 0.9000 - 191s/epoch - 6s/step\n",
            "Epoch 20/20\n",
            "30/30 - 193s - loss: 0.9768 - accuracy: 0.8500 - val_loss: 0.6826 - val_accuracy: 0.8800 - 193s/epoch - 6s/step\n",
            "[INFO] Converting Data into Image form and encoding labels\n",
            "[INFO] Getting Predictions\n",
            "[INFO] Training model  effnet_b2\n",
            "Epoch 1/20\n",
            "30/30 - 172s - loss: 2.1583 - accuracy: 0.7000 - val_loss: 0.0012 - val_accuracy: 1.0000 - 172s/epoch - 6s/step\n",
            "Epoch 2/20\n",
            "30/30 - 144s - loss: 1.6905 - accuracy: 0.8133 - val_loss: 0.0046 - val_accuracy: 1.0000 - 144s/epoch - 5s/step\n",
            "Epoch 3/20\n",
            "30/30 - 145s - loss: 1.5776 - accuracy: 0.7867 - val_loss: 1.4215e-04 - val_accuracy: 1.0000 - 145s/epoch - 5s/step\n",
            "Epoch 4/20\n",
            "30/30 - 148s - loss: 3.0674 - accuracy: 0.7833 - val_loss: 8.5070e-04 - val_accuracy: 1.0000 - 148s/epoch - 5s/step\n",
            "Epoch 5/20\n",
            "30/30 - 143s - loss: 1.7603 - accuracy: 0.8167 - val_loss: 2.8849e-07 - val_accuracy: 1.0000 - 143s/epoch - 5s/step\n",
            "Epoch 6/20\n",
            "30/30 - 145s - loss: 1.2881 - accuracy: 0.8300 - val_loss: 9.1340e-04 - val_accuracy: 1.0000 - 145s/epoch - 5s/step\n",
            "Epoch 7/20\n",
            "30/30 - 142s - loss: 1.6410 - accuracy: 0.8433 - val_loss: 7.2002e-07 - val_accuracy: 1.0000 - 142s/epoch - 5s/step\n",
            "Epoch 8/20\n",
            "30/30 - 144s - loss: 2.0890 - accuracy: 0.8000 - val_loss: 0.0454 - val_accuracy: 0.9800 - 144s/epoch - 5s/step\n",
            "Epoch 9/20\n",
            "30/30 - 144s - loss: 1.5276 - accuracy: 0.8567 - val_loss: 0.0324 - val_accuracy: 0.9800 - 144s/epoch - 5s/step\n",
            "Epoch 10/20\n",
            "30/30 - 141s - loss: 1.4237 - accuracy: 0.8533 - val_loss: 0.2094 - val_accuracy: 0.9600 - 141s/epoch - 5s/step\n",
            "Epoch 11/20\n",
            "30/30 - 142s - loss: 1.6809 - accuracy: 0.8533 - val_loss: 0.0041 - val_accuracy: 1.0000 - 142s/epoch - 5s/step\n",
            "Epoch 12/20\n",
            "30/30 - 142s - loss: 1.9578 - accuracy: 0.8100 - val_loss: 0.0031 - val_accuracy: 1.0000 - 142s/epoch - 5s/step\n",
            "Epoch 13/20\n",
            "30/30 - 138s - loss: 1.7553 - accuracy: 0.8433 - val_loss: 0.1435 - val_accuracy: 0.9600 - 138s/epoch - 5s/step\n",
            "Epoch 14/20\n",
            "30/30 - 140s - loss: 2.9061 - accuracy: 0.8000 - val_loss: 7.2955e-07 - val_accuracy: 1.0000 - 140s/epoch - 5s/step\n",
            "Epoch 15/20\n",
            "30/30 - 137s - loss: 1.6640 - accuracy: 0.8867 - val_loss: 1.2683e-06 - val_accuracy: 1.0000 - 137s/epoch - 5s/step\n",
            "Epoch 16/20\n",
            "30/30 - 139s - loss: 1.0402 - accuracy: 0.8533 - val_loss: 1.5321e-05 - val_accuracy: 1.0000 - 139s/epoch - 5s/step\n",
            "Epoch 17/20\n",
            "30/30 - 140s - loss: 1.9329 - accuracy: 0.8800 - val_loss: 0.0093 - val_accuracy: 1.0000 - 140s/epoch - 5s/step\n",
            "Epoch 18/20\n",
            "30/30 - 139s - loss: 2.0143 - accuracy: 0.8567 - val_loss: 0.0047 - val_accuracy: 1.0000 - 139s/epoch - 5s/step\n",
            "Epoch 19/20\n",
            "30/30 - 140s - loss: 3.0159 - accuracy: 0.8100 - val_loss: 0.0302 - val_accuracy: 0.9800 - 140s/epoch - 5s/step\n",
            "Epoch 20/20\n",
            "30/30 - 138s - loss: 2.1718 - accuracy: 0.8333 - val_loss: 0.1031 - val_accuracy: 0.9800 - 138s/epoch - 5s/step\n",
            "[INFO] Converting Data into Image form and encoding labels\n",
            "[INFO] Getting Predictions\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/NASNet-large-no-top.h5\n",
            "343613440/343610240 [==============================] - 4s 0us/step\n",
            "343621632/343610240 [==============================] - 4s 0us/step\n",
            "[INFO] Training model  nas_net\n",
            "Epoch 1/20\n",
            "30/30 - 195s - loss: 4.3586 - accuracy: 0.6300 - val_loss: 0.0024 - val_accuracy: 1.0000 - 195s/epoch - 6s/step\n",
            "Epoch 2/20\n",
            "30/30 - 169s - loss: 3.2730 - accuracy: 0.6733 - val_loss: 0.0115 - val_accuracy: 1.0000 - 169s/epoch - 6s/step\n",
            "Epoch 3/20\n",
            "30/30 - 172s - loss: 3.3999 - accuracy: 0.7033 - val_loss: 0.1900 - val_accuracy: 0.9000 - 172s/epoch - 6s/step\n",
            "Epoch 4/20\n",
            "30/30 - 176s - loss: 2.7936 - accuracy: 0.7033 - val_loss: 0.7892 - val_accuracy: 0.8200 - 176s/epoch - 6s/step\n",
            "Epoch 5/20\n",
            "30/30 - 171s - loss: 3.0091 - accuracy: 0.7033 - val_loss: 2.7700 - val_accuracy: 0.5800 - 171s/epoch - 6s/step\n",
            "Epoch 6/20\n",
            "30/30 - 171s - loss: 2.9212 - accuracy: 0.6667 - val_loss: 0.4614 - val_accuracy: 0.9400 - 171s/epoch - 6s/step\n",
            "Epoch 7/20\n",
            "30/30 - 168s - loss: 4.1908 - accuracy: 0.6500 - val_loss: 1.1923 - val_accuracy: 0.6600 - 168s/epoch - 6s/step\n",
            "Epoch 8/20\n",
            "30/30 - 170s - loss: 3.9024 - accuracy: 0.6733 - val_loss: 3.8173 - val_accuracy: 0.5400 - 170s/epoch - 6s/step\n",
            "Epoch 9/20\n",
            "30/30 - 171s - loss: 3.6560 - accuracy: 0.6200 - val_loss: 0.8146 - val_accuracy: 0.8000 - 171s/epoch - 6s/step\n",
            "Epoch 10/20\n",
            "30/30 - 175s - loss: 3.5981 - accuracy: 0.6233 - val_loss: 2.6515 - val_accuracy: 0.7800 - 175s/epoch - 6s/step\n",
            "Epoch 11/20\n",
            "30/30 - 170s - loss: 3.7277 - accuracy: 0.6933 - val_loss: 1.3281 - val_accuracy: 0.7200 - 170s/epoch - 6s/step\n",
            "Epoch 12/20\n",
            "30/30 - 172s - loss: 2.5248 - accuracy: 0.7300 - val_loss: 0.5057 - val_accuracy: 0.9200 - 172s/epoch - 6s/step\n",
            "Epoch 13/20\n",
            "30/30 - 172s - loss: 3.0000 - accuracy: 0.6567 - val_loss: 0.5709 - val_accuracy: 0.8600 - 172s/epoch - 6s/step\n",
            "Epoch 14/20\n",
            "30/30 - 174s - loss: 4.0233 - accuracy: 0.6633 - val_loss: 0.2254 - val_accuracy: 0.9400 - 174s/epoch - 6s/step\n",
            "Epoch 15/20\n",
            "30/30 - 172s - loss: 2.9291 - accuracy: 0.7167 - val_loss: 1.2303 - val_accuracy: 0.7800 - 172s/epoch - 6s/step\n",
            "Epoch 16/20\n",
            "30/30 - 170s - loss: 2.9078 - accuracy: 0.6733 - val_loss: 1.4118 - val_accuracy: 0.7400 - 170s/epoch - 6s/step\n",
            "Epoch 17/20\n",
            "30/30 - 172s - loss: 2.9595 - accuracy: 0.6967 - val_loss: 2.0251 - val_accuracy: 0.7200 - 172s/epoch - 6s/step\n",
            "Epoch 18/20\n",
            "30/30 - 173s - loss: 2.4231 - accuracy: 0.7067 - val_loss: 0.3620 - val_accuracy: 0.9200 - 173s/epoch - 6s/step\n",
            "Epoch 19/20\n",
            "30/30 - 173s - loss: 2.6626 - accuracy: 0.6833 - val_loss: 3.4336 - val_accuracy: 0.6600 - 173s/epoch - 6s/step\n",
            "Epoch 20/20\n",
            "30/30 - 172s - loss: 4.0524 - accuracy: 0.6933 - val_loss: 1.7604 - val_accuracy: 0.6600 - 172s/epoch - 6s/step\n",
            "[INFO] Converting Data into Image form and encoding labels\n",
            "[INFO] Getting Predictions\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 2s 0us/step\n",
            "219070464/219055592 [==============================] - 2s 0us/step\n",
            "[INFO] Training model  inc_resnet_v2\n",
            "Epoch 1/20\n",
            "30/30 - 102s - loss: 2.9850 - accuracy: 0.5833 - val_loss: 1.3280 - val_accuracy: 0.6200 - 102s/epoch - 3s/step\n",
            "Epoch 2/20\n",
            "30/30 - 88s - loss: 1.8656 - accuracy: 0.5867 - val_loss: 3.8744 - val_accuracy: 0.6000 - 88s/epoch - 3s/step\n",
            "Epoch 3/20\n",
            "30/30 - 85s - loss: 2.6392 - accuracy: 0.5833 - val_loss: 0.1315 - val_accuracy: 0.9200 - 85s/epoch - 3s/step\n",
            "Epoch 4/20\n",
            "30/30 - 89s - loss: 1.9781 - accuracy: 0.5667 - val_loss: 0.3899 - val_accuracy: 0.8000 - 89s/epoch - 3s/step\n",
            "Epoch 5/20\n",
            "30/30 - 86s - loss: 1.3706 - accuracy: 0.6400 - val_loss: 1.1406 - val_accuracy: 0.4000 - 86s/epoch - 3s/step\n",
            "Epoch 6/20\n",
            "30/30 - 87s - loss: 1.4148 - accuracy: 0.5933 - val_loss: 2.9987 - val_accuracy: 0.7400 - 87s/epoch - 3s/step\n",
            "Epoch 7/20\n",
            "30/30 - 91s - loss: 2.9366 - accuracy: 0.5800 - val_loss: 4.5534 - val_accuracy: 0.6600 - 91s/epoch - 3s/step\n",
            "Epoch 8/20\n",
            "30/30 - 86s - loss: 1.5816 - accuracy: 0.5800 - val_loss: 1.1216 - val_accuracy: 0.6200 - 86s/epoch - 3s/step\n",
            "Epoch 9/20\n",
            "30/30 - 89s - loss: 1.4875 - accuracy: 0.5700 - val_loss: 0.7340 - val_accuracy: 0.8200 - 89s/epoch - 3s/step\n",
            "Epoch 10/20\n",
            "30/30 - 86s - loss: 2.1397 - accuracy: 0.5633 - val_loss: 0.4686 - val_accuracy: 0.7600 - 86s/epoch - 3s/step\n",
            "Epoch 11/20\n",
            "30/30 - 88s - loss: 1.5566 - accuracy: 0.6100 - val_loss: 1.0584 - val_accuracy: 0.6400 - 88s/epoch - 3s/step\n",
            "Epoch 12/20\n",
            "30/30 - 86s - loss: 1.2362 - accuracy: 0.6333 - val_loss: 1.2300 - val_accuracy: 0.8000 - 86s/epoch - 3s/step\n",
            "Epoch 13/20\n",
            "30/30 - 85s - loss: 1.9504 - accuracy: 0.6200 - val_loss: 1.3132 - val_accuracy: 0.5000 - 85s/epoch - 3s/step\n",
            "Epoch 14/20\n",
            "30/30 - 89s - loss: 1.8193 - accuracy: 0.6033 - val_loss: 1.7338 - val_accuracy: 0.5800 - 89s/epoch - 3s/step\n",
            "Epoch 15/20\n",
            "30/30 - 86s - loss: 1.9876 - accuracy: 0.5600 - val_loss: 3.5231 - val_accuracy: 0.1800 - 86s/epoch - 3s/step\n",
            "Epoch 16/20\n",
            "30/30 - 89s - loss: 1.3743 - accuracy: 0.6133 - val_loss: 1.6369 - val_accuracy: 0.4000 - 89s/epoch - 3s/step\n",
            "Epoch 17/20\n",
            "30/30 - 86s - loss: 1.6206 - accuracy: 0.5700 - val_loss: 0.5700 - val_accuracy: 0.8200 - 86s/epoch - 3s/step\n",
            "Epoch 18/20\n",
            "30/30 - 89s - loss: 1.1020 - accuracy: 0.6133 - val_loss: 0.4108 - val_accuracy: 0.8600 - 89s/epoch - 3s/step\n",
            "Epoch 19/20\n",
            "30/30 - 86s - loss: 1.2211 - accuracy: 0.5867 - val_loss: 0.2505 - val_accuracy: 0.9200 - 86s/epoch - 3s/step\n",
            "Epoch 20/20\n",
            "30/30 - 86s - loss: 1.5502 - accuracy: 0.5700 - val_loss: 0.3241 - val_accuracy: 0.8600 - 86s/epoch - 3s/step\n",
            "[INFO] Converting Data into Image form and encoding labels\n",
            "[INFO] Getting Predictions\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb434aa2680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb434aa2680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Effnet_v2l\")\n",
        "model_performance(predictions=all_results['effnet_v2l']['predictions'], true_labels = all_results['effnet_v2l']['true_labels'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "r7hhEIlPIDVk",
        "outputId": "2dfbb3e0-7a66-4d2b-ee41-eae019b52a2c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effnet_v2l\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1e63b876c033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Effnet_v2l\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'effnet_v2l'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'effnet_v2l'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'true_labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model_performance' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ty6Yg-J6h6ZO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}